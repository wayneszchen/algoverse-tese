{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai>=1.30 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.97.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (3.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (2.11.7)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from openai>=1.30) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>=1.30) (3.4)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.30) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>=1.30) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.30) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.30) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.30) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai>=1.30) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# If needed, run this once (comment out if your env already has these)\n",
    "# Note: may require restart after install in some notebook environments.\n",
    "!pip install --upgrade \"openai>=1.30\" pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI v1.x client ready.\n"
     ]
    }
   ],
   "source": [
    "# Robust OpenAI client init for Jupyter (v1.x SDK)\n",
    "# - Sanitizes pasted key (removes spaces/newlines/zero-width chars)\n",
    "# - Does not hard-fail on prefix; only warns\n",
    "\n",
    "import os, sys, re\n",
    "from getpass import getpass\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Any, Optional, Dict\n",
    "import json, random\n",
    "\n",
    "def _sanitize_key(k: str) -> str:\n",
    "    if not isinstance(k, str):\n",
    "        return \"\"\n",
    "    # remove all whitespace (spaces, tabs, newlines) and zero-width chars\n",
    "    k = k.replace(\"\\u200b\", \"\").replace(\"\\u200c\", \"\").replace(\"\\u200d\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    k = \"\".join(k.split())\n",
    "    return k\n",
    "# Minimal, robust init for OpenAI v1.x in Jupyter\n",
    "# - Reads OPENAI_API_KEY from the environment\n",
    "# - Strips stray whitespace characters\n",
    "# - Uses the modern client pattern\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "def _sanitize_key(k: str) -> str:\n",
    "    if not isinstance(k, str):\n",
    "        return \"\"\n",
    "    for z in (\"\\u200b\", \"\\u200c\", \"\\u200d\", \"\\ufeff\"):\n",
    "        k = k.replace(z, \"\")\n",
    "    return \"\".join(k.split())\n",
    "\n",
    "# OpenAI client init (env var first, fallback to hardcoded string) — v1.x compatible\n",
    "# If you set OPENAI_API_KEY in your terminal/.env, you don't need the fallback.\n",
    "\n",
    "import os, sys\n",
    "import openai\n",
    "from openai import OpenAI  # v1.x client class\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    openai.api_key = openai_api_key.strip()\n",
    "else:\n",
    "    # =======================  << PASTE HERE >>  =======================\n",
    "    # Replace the placeholder below with YOUR real key if not using env var.\n",
    "    # Example (FAKE): \"sk-proj-EXAMPLE1234567890abcdef...\"\n",
    "    openai.api_key = \"\"  # <-- paste your key here\n",
    "    # ==================================================================\n",
    "\n",
    "# Basic validation\n",
    "if not openai.api_key or openai.api_key.strip() == \"\" or openai.api_key.startswith(\"YOUR-\"):\n",
    "    print(\"Error: OpenAI API key not provided.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Create the v1.x client using the key we set above\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "print(\"✅ OpenAI v1.x client ready.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('runs/20250727_184519')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Paths ===\n",
    "INPUT_JSON = \"prompt_test_2_grouped.json\"  # put this next to the notebook or use absolute path\n",
    "RUNS_ROOT  = Path(\"runs\")\n",
    "\n",
    "# === Models ===\n",
    "GEN_MODEL   = \"gpt-4o\"   # for generation\n",
    "JUDGE_MODEL = \"gpt-4o\"   # for judging\n",
    "\n",
    "# === Generation params ===\n",
    "GEN_TEMPERATURE = 0.4\n",
    "GEN_MAX_TOKENS  = 800    # be realistic; too high can error\n",
    "GEN_RETRIES     = 5\n",
    "\n",
    "# === Judge params ===\n",
    "JUDGE_TEMPERATURE = 0.0\n",
    "JUDGE_MAX_TOKENS  = 350\n",
    "JUDGE_RETRIES     = 5\n",
    "\n",
    "# === Fresh run dir ===\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "RUN_DIR = RUNS_ROOT / ts\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Output artifact paths\n",
    "PATH_WITH_ANSWERS = RUN_DIR / \"with_answers.json\"\n",
    "PATH_WITH_SCORES  = RUN_DIR / \"with_scores.json\"\n",
    "PATH_PER_PROMPT   = RUN_DIR / \"per_prompt_scores.csv\"\n",
    "PATH_SUMMARY      = RUN_DIR / \"summary_by_type.csv\"\n",
    "PATH_TRIPLETS     = RUN_DIR / \"triplets_by_topic.csv\"\n",
    "PATH_GEN_LOG      = RUN_DIR / \"gen_log.jsonl\"\n",
    "PATH_JUDGE_LOG    = RUN_DIR / \"judge_log.jsonl\"\n",
    "\n",
    "RUN_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Utilities ready.\n"
     ]
    }
   ],
   "source": [
    "def backoff_wait(attempt: int) -> float:\n",
    "    \"\"\"Exponential backoff with jitter, capped at 60s.\"\"\"\n",
    "    return min(60.0, (2 ** attempt) + random.random())\n",
    "\n",
    "def load_json(path: str) -> Any:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def append_jsonl(path: Path, obj: Any) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def usage_dict(resp) -> Optional[dict]:\n",
    "    try:\n",
    "        u = resp.usage\n",
    "        return {\n",
    "            \"prompt_tokens\": getattr(u, \"prompt_tokens\", None),\n",
    "            \"completion_tokens\": getattr(u, \"completion_tokens\", None),\n",
    "            \"total_tokens\": getattr(u, \"total_tokens\", None),\n",
    "        }\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "print(\"✅ Utilities ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 60 prompts.\n",
      "Example: {'id': 'Neutral-01', 'type': 'Neutral', 'topic': 'Industrial Revolution', 'question': 'Write a balanced, well‑sourced essay explaining the catalysts, mechanisms, and social–economic consequences of the Industrial Revolution.', 'model_answer': ''}\n"
     ]
    }
   ],
   "source": [
    "data = load_json(INPUT_JSON)\n",
    "items = data.get(\"outputs\") or []\n",
    "print(f\"Loaded {len(items)} prompts.\")\n",
    "print(\"Example:\", items[0] if items else \"N/A\")\n",
    "\n",
    "# Ensure we start clean: drop any lingering model_answer/evaluation in-memory if present\n",
    "for it in items:\n",
    "    it[\"model_answer\"] = \"\"\n",
    "    if \"evaluation\" in it:\n",
    "        del it[\"evaluation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check: READY\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a concise assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Reply with exactly READY\"}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=5,\n",
    "    )\n",
    "    print(\"Sanity check:\", test.choices[0].message.content.strip())\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"❌ Sanity check failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/60] Generating: Neutral-01\n",
      "[2/60] Generating: Supportive-01\n",
      "[3/60] Generating: Threatening-01\n",
      "[4/60] Generating: Neutral-02\n",
      "[5/60] Generating: Supportive-02\n",
      "[6/60] Generating: Threatening-02\n",
      "[7/60] Generating: Neutral-03\n",
      "[8/60] Generating: Supportive-03\n",
      "[9/60] Generating: Threatening-03\n",
      "[10/60] Generating: Neutral-04\n",
      "[11/60] Generating: Supportive-04\n",
      "[12/60] Generating: Threatening-04\n",
      "[13/60] Generating: Neutral-05\n",
      "[14/60] Generating: Supportive-05\n",
      "[15/60] Generating: Threatening-05\n",
      "[16/60] Generating: Neutral-06\n",
      "[17/60] Generating: Supportive-06\n",
      "[18/60] Generating: Threatening-06\n",
      "[19/60] Generating: Neutral-07\n",
      "[20/60] Generating: Supportive-07\n",
      "[21/60] Generating: Threatening-07\n",
      "[22/60] Generating: Neutral-08\n",
      "[23/60] Generating: Supportive-08\n",
      "[24/60] Generating: Threatening-08\n",
      "[25/60] Generating: Neutral-09\n",
      "[26/60] Generating: Supportive-09\n",
      "[27/60] Generating: Threatening-09\n",
      "[28/60] Generating: Neutral-10\n",
      "[29/60] Generating: Supportive-10\n",
      "[30/60] Generating: Threatening-10\n",
      "[31/60] Generating: Neutral-11\n",
      "[32/60] Generating: Supportive-11\n",
      "[33/60] Generating: Threatening-11\n",
      "[34/60] Generating: Neutral-12\n",
      "[35/60] Generating: Supportive-12\n",
      "[36/60] Generating: Threatening-12\n",
      "[37/60] Generating: Neutral-13\n",
      "[38/60] Generating: Supportive-13\n",
      "[39/60] Generating: Threatening-13\n",
      "[40/60] Generating: Neutral-14\n",
      "[41/60] Generating: Supportive-14\n",
      "[42/60] Generating: Threatening-14\n",
      "[43/60] Generating: Neutral-15\n",
      "[44/60] Generating: Supportive-15\n",
      "[45/60] Generating: Threatening-15\n",
      "[46/60] Generating: Neutral-16\n",
      "[47/60] Generating: Supportive-16\n",
      "[48/60] Generating: Threatening-16\n",
      "[49/60] Generating: Neutral-17\n",
      "[50/60] Generating: Supportive-17\n",
      "[51/60] Generating: Threatening-17\n",
      "[52/60] Generating: Neutral-18\n",
      "[53/60] Generating: Supportive-18\n",
      "[54/60] Generating: Threatening-18\n",
      "[55/60] Generating: Neutral-19\n",
      "[56/60] Generating: Supportive-19\n",
      "[57/60] Generating: Threatening-19\n",
      "[58/60] Generating: Neutral-20\n",
      "[59/60] Generating: Supportive-20\n",
      "[60/60] Generating: Threatening-20\n",
      "\n",
      "✅ Generation complete. 60 answers written.\n",
      "Saved → runs/20250727_175338/with_answers.json\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_WRITER = (\n",
    "    \"You are a careful, well-sourced academic writer. \"\n",
    "    \"Answer comprehensively, clearly, and factually. Use concise paragraphs.\"\n",
    ")\n",
    "\n",
    "def generate_one(question: str) -> str:\n",
    "    last_err = None\n",
    "    for attempt in range(GEN_RETRIES):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=GEN_MODEL,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_WRITER},\n",
    "                    {\"role\": \"user\", \"content\": question},\n",
    "                ],\n",
    "                temperature=GEN_TEMPERATURE,\n",
    "                max_tokens=GEN_MAX_TOKENS,\n",
    "            )\n",
    "            return (resp.choices[0].message.content or \"\").strip(), resp\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            wait = backoff_wait(attempt)\n",
    "            print(f\"  ⚠️ gen retry {attempt+1}/{GEN_RETRIES}: {e} (sleep {wait:.1f}s)\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Generation failed after {GEN_RETRIES} retries: {last_err}\")\n",
    "\n",
    "generated = 0\n",
    "for i, it in enumerate(items, 1):\n",
    "    pid = it.get(\"id\", f\"index_{i}\")\n",
    "    q   = it.get(\"question\", \"\")\n",
    "    print(f\"[{i}/{len(items)}] Generating: {pid}\")\n",
    "    try:\n",
    "        answer, resp = generate_one(q)\n",
    "        it[\"model_answer\"] = answer\n",
    "        generated += 1\n",
    "        append_jsonl(PATH_GEN_LOG, {\n",
    "            \"ts\": datetime.utcnow().isoformat()+\"Z\",\n",
    "            \"id\": pid,\n",
    "            \"type\": it.get(\"type\"),\n",
    "            \"topic\": it.get(\"topic\"),\n",
    "            \"usage\": usage_dict(resp),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        it[\"model_answer\"] = f\"<ERROR: {e}>\"\n",
    "        print(f\"   ✗ error: {e}\")\n",
    "\n",
    "# Save answers\n",
    "save_json(PATH_WITH_ANSWERS, {\"outputs\": items})\n",
    "print(f\"\\n✅ Generation complete. {generated} answers written.\")\n",
    "print(f\"Saved → {PATH_WITH_ANSWERS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed when ready.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to block until you review the JSON\n",
    "# input(f\"Review answers at: {PATH_WITH_ANSWERS}\\nPress Enter to proceed to judging ... \")\n",
    "print(\"Proceed when ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the following with_answers.json files (newest first):\n",
      "\n",
      "[1] runs/20250727_175338/with_answers.json  | items=60  ok=60  empty=0  errors=0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "RUNS_ROOT = Path(\"runs\")  # adjust if your runs folder is elsewhere\n",
    "\n",
    "def count_judgeable(path: Path):\n",
    "    try:\n",
    "        data = json.loads(Path(path).read_text(encoding=\"utf-8\"))\n",
    "        items = data.get(\"outputs\", [])\n",
    "        empty = sum(1 for it in items if not (it.get(\"model_answer\") or \"\").strip())\n",
    "        errors = sum(1 for it in items if (it.get(\"model_answer\") or \"\").strip().startswith(\"<ERROR\"))\n",
    "        ok = len(items) - empty - errors\n",
    "        return len(items), ok, empty, errors\n",
    "    except Exception as e:\n",
    "        return None, None, None, None\n",
    "\n",
    "cands = sorted(RUNS_ROOT.glob(\"*/with_answers.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "if not cands:\n",
    "    print(\"No runs/*/with_answers.json found. You may have saved to another folder.\")\n",
    "else:\n",
    "    print(\"Found the following with_answers.json files (newest first):\\n\")\n",
    "    for i, p in enumerate(cands, 1):\n",
    "        total, ok, empty, errors = count_judgeable(p)\n",
    "        print(f\"[{i}] {p}  | items={total}  ok={ok}  empty={empty}  errors={errors}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No valid answers found. Re-run generation first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Pick the first item that has a valid answer\u001b[39;00m\n\u001b[1;32m      5\u001b[0m probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m((it \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m items \u001b[38;5;28;01mif\u001b[39;00m (it\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m it[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_answer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<ERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m probe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid answers found. Re-run generation first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbing\u001b[39m\u001b[38;5;124m\"\u001b[39m, probe[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m RUBRIC_KEYS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelevance_task\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactual_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreativity_originality\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m ]\n",
      "\u001b[0;31mAssertionError\u001b[0m: No valid answers found. Re-run generation first."
     ]
    }
   ],
   "source": [
    "import json, time, random, re\n",
    "from datetime import datetime\n",
    "\n",
    "# Pick the first item that has a valid answer\n",
    "probe = next((it for it in items if (it.get(\"model_answer\") or \"\").strip() and not it[\"model_answer\"].startswith(\"<ERROR\")), None)\n",
    "assert probe is not None, \"No valid answers found. Re-run generation first.\"\n",
    "\n",
    "print(\"Probing\", probe[\"id\"])\n",
    "\n",
    "RUBRIC_KEYS = [\n",
    "    \"relevance_task\",\n",
    "    \"factual_accuracy\",\n",
    "    \"coherence_structure\",\n",
    "    \"depth_insight\",\n",
    "    \"linguistic_quality\",\n",
    "    \"instruction_sensitivity\",\n",
    "    \"creativity_originality\",\n",
    "]\n",
    "\n",
    "RUBRIC_JSON_SCHEMA = {\n",
    "    \"name\": \"rubric_scores\",\n",
    "    \"strict\": True,\n",
    "    \"schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"additionalProperties\": False,\n",
    "        \"required\": [\"scores\", \"total\", \"comments\"],\n",
    "        \"properties\": {\n",
    "            \"scores\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": False,\n",
    "                \"required\": RUBRIC_KEYS,\n",
    "                \"properties\": {k: {\"type\": \"number\", \"minimum\": 0, \"maximum\": 5} for k in RUBRIC_KEYS},\n",
    "            },\n",
    "            \"total\":    {\"type\": \"number\", \"minimum\": 0, \"maximum\": 35},\n",
    "            \"comments\": {\"type\": \"string\"},\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "def _to2(x: float) -> float:\n",
    "    return float(f\"{x:.2f}\")\n",
    "\n",
    "def judge_one_schema_probe(client, question: str, answer: str):\n",
    "    JUDGE_SYSTEM = \"You are an expert evaluator. Output valid JSON that strictly matches the provided schema.\"\n",
    "    JUDGE_USER = f\"\"\"Score the ANSWER to the QUESTION using the 7-category rubric below.\n",
    "Each category must be a float from 0.00 to 5.00 (two decimals). Total is the sum (0.00–35.00).\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\"\"\".strip()\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
    "            {\"role\": \"user\", \"content\": JUDGE_USER},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=400,\n",
    "        response_format={\"type\": \"json_schema\", \"json_schema\": RUBRIC_JSON_SCHEMA},\n",
    "    )\n",
    "    raw = (resp.choices[0].message.content or \"\").strip()\n",
    "    print(\"Raw JSON returned:\\n\", raw[:400], \"...\\n\")  # show a preview\n",
    "\n",
    "    data = json.loads(raw)  # should conform to schema\n",
    "\n",
    "    scores = {k: _to2(float(data[\"scores\"][k])) for k in RUBRIC_KEYS}\n",
    "    total  = _to2(float(data.get(\"total\", sum(scores.values()))))\n",
    "    return {\"scores\": scores, \"total\": total, \"comments\": data.get(\"comments\", \"\")}\n",
    "\n",
    "res = judge_one_schema_probe(client, probe[\"question\"], probe[\"model_answer\"])\n",
    "print(\"Parsed:\\n\", res)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PATH_WITH_SCORES = RUN_DIR / \"with_scores.json\"\n",
    "RAW_JUDGE_LOG = RUN_DIR / \"judge_raw.jsonl\"\n",
    "\n",
    "def _to2(x: float) -> float:\n",
    "    return float(f\"{x:.2f}\")\n",
    "\n",
    "def backoff(attempt: int) -> float:\n",
    "    return min(60.0, (2**attempt) + random.random())\n",
    "\n",
    "def _log_raw(obj: dict):\n",
    "    with open(RAW_JUDGE_LOG, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def judge_one_schema(client, question: str, answer: str, retries: int = 4):\n",
    "    JUDGE_SYSTEM = \"You are an expert evaluator. Output valid JSON that strictly matches the provided schema.\"\n",
    "    JUDGE_USER = f\"\"\"Score the ANSWER to the QUESTION using the 7-category rubric below.\n",
    "Each category must be a float from 0.00 to 5.00 (two decimals). Total is the sum (0.00–35.00).\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "{answer}\n",
    "\"\"\".strip()\n",
    "\n",
    "    last_err = None\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": JUDGE_SYSTEM},\n",
    "                    {\"role\": \"user\", \"content\": JUDGE_USER},\n",
    "                ],\n",
    "                temperature=0.0,\n",
    "                max_tokens=400,\n",
    "                response_format={\"type\": \"json_schema\", \"json_schema\": RUBRIC_JSON_SCHEMA},\n",
    "            )\n",
    "            raw = (resp.choices[0].message.content or \"\").strip()\n",
    "            _log_raw({\"ok\": True, \"raw\": raw})\n",
    "\n",
    "            data = json.loads(raw)\n",
    "            scores = {k: _to2(float(data[\"scores\"][k])) for k in RUBRIC_KEYS}\n",
    "            total  = _to2(float(data.get(\"total\", sum(scores.values()))))\n",
    "            comments = data.get(\"comments\", \"\")\n",
    "            if not isinstance(comments, str):\n",
    "                comments = str(comments)\n",
    "            return {\"scores\": scores, \"total\": total, \"comments\": comments}\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            _log_raw({\"ok\": False, \"error\": str(e)})\n",
    "            wait = backoff(attempt)\n",
    "            print(f\"  ⚠️ judge retry {attempt+1}/{retries}: {e} (sleep {wait:.1f}s)\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Judging failed after retries: {last_err}\")\n",
    "\n",
    "judged = 0\n",
    "for i, it in enumerate(items, 1):\n",
    "    pid = it.get(\"id\", f\"index_{i}\")\n",
    "    q   = (it.get(\"question\") or \"\").strip()\n",
    "    a   = (it.get(\"model_answer\") or \"\").strip()\n",
    "\n",
    "    if not a or a.startswith(\"<ERROR\"):\n",
    "        print(f\"[{i}/{len(items)}] {pid}: no valid answer; skipping judge.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"[{i}/{len(items)}] Judging: {pid}\")\n",
    "    try:\n",
    "        result = judge_one_schema(client, q, a, retries=3)\n",
    "        it[\"evaluation\"] = result\n",
    "        judged += 1\n",
    "        print(f\"   ✓ total={result['total']:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ✗ judge error for {pid}: {e}\")\n",
    "\n",
    "with open(PATH_WITH_SCORES, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"outputs\": items}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Judging complete for {judged} items.\")\n",
    "print(f\"Saved → {PATH_WITH_SCORES}\")\n",
    "print(f\"Raw judge replies logged → {RAW_JUDGE_LOG}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
